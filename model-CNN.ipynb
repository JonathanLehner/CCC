{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model using built-in library from Pytorch.\n",
    "This code closely follows nn_tutorial notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, torch, math, os, pickle\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "global chroma_shape, epochs, train_bs, validate_bs, lr, n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read attr and tar in .pkl files\n",
    "## Param: ratio = training size to the sample size. \n",
    "##         E.g. 0.7 means using 70% of the samples as training \n",
    "##              sample and the rest as validation sample. \n",
    "## Return: (1) a list of padded attr arrays\n",
    "##         (2) a list of paddrd tar arrays\n",
    "def read_data(ratio):\n",
    "    \n",
    "    assert (ratio > 0 and ratio < 1), 'Invalid ratio'\n",
    "    \n",
    "    \n",
    "    att_file = open(r'mix_chroma_attr.pkl', 'rb')\n",
    "    x_train = pickle.load(att_file)  \n",
    "    att_file.close()\n",
    "    \n",
    "    tar_file = open(r'mix_chroma_tar.pkl', 'rb')\n",
    "    str_y_train = pickle.load(tar_file)\n",
    "    tar_file.close()\n",
    "    \n",
    "    y = torch.tensor([0 if s == 's' else (1 if s == 'x' else 2) for s in str_y_train])\n",
    "    assert (len(x_train) == len(y)), 'Unequal sample lengths'\n",
    "    \n",
    "    #print([(x,s) for x, s in zip(str_y_train, y_train)])\n",
    "    \n",
    "    ## need .float so that it has the same type as weights in the model\n",
    "    #return torch.tensor(x_train).float(), y_train\n",
    "    x = torch.tensor(x_train).float() \n",
    "    divider = int(len(y)*ratio)\n",
    "    \n",
    "    \n",
    "    x_train = x[:divider, :, :]\n",
    "    y_train = y[:divider]\n",
    "    \n",
    "    print(x_train.shape, x.shape)\n",
    "\n",
    "    x_validate = x[divider:, :, :]\n",
    "    y_validate = y[divider:]\n",
    "\n",
    "    assert (x_train.shape[0] + x_validate.shape[0] == len(y_train) + len(y_validate)), 'Lengths do not add up'\n",
    "\n",
    "    return x_train, y_train, x_validate, y_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1646, 12, 60]) torch.Size([2352, 12, 60])\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_validate, y_validate = read_data(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1646"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    ##get the index with the max\n",
    "    preds = torch.argmax(out, dim = 1)\n",
    "    return (preds == yb).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the shape of a padded instance for model construction\n",
    "chroma_shape = x_train[0].shape\n",
    "\n",
    "lr = 0.00001\n",
    "epochs = 1000\n",
    "train_bs = 20\n",
    "validate_bs = train_bs*2\n",
    "n_train = x_train.shape[0]\n",
    "n_validate = x_validate.shape[0]\n",
    "n_class = 3\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are three classes, we set D_out to 3. n is total number of instances and c is the number of attributes in each instance. We use a loss function from torch.nn.functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundRecognition_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = torch.nn.Linear(16*3*27, n_class)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        #print(type(xb), len(xb))\n",
    "        \n",
    "        ## input of size (bs, 1, 12, 60)\n",
    "        xb = xb.view(-1, 1, chroma_shape[0], chroma_shape[1])\n",
    "        #print('xb shape', xb.shape)\n",
    "        \n",
    "        ## (bs, 1, 12, 60) >> (bs, 16, 10, 58)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        #print('---xb shape2', xb.shape)\n",
    "        \n",
    "        ## (bs, 16, 10, 58) >> (bs, 16, 8, 56)\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        #print('---xb shape3', xb.shape)\n",
    "        \n",
    "        ## (bs, 16, 8, 56) >> (bs, 16, 6, 54)\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        #print('---xb shape4', xb.shape)\n",
    "        \n",
    "        ## (bs, 16, 6, 54) >> (bs, 16, 3, 27)\n",
    "        xb = self.pool(xb)\n",
    "        #print('---xb shape5', xb.shape)\n",
    "        \n",
    "        ## reshape for fully connected\n",
    "        xb = xb.view(-1, 16*3*27)\n",
    "        \n",
    "        ## (bs, 16*3*27) >> (bs, 3)\n",
    "        xb = self.fc1(xb)\n",
    "        #print('---xb shape6', xb.shape)\n",
    "        #print('=====', xb.shape)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "\n",
    "## Get the model and optim object that will be used to update model parameters\n",
    "def get_model():\n",
    "    model = SoundRecognition_CNN()\n",
    "    return model, optim.SGD(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(trained_model):\n",
    "    trained_model.eval()\n",
    "    loss = []\n",
    "    acc = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range((n_validate - 1) // validate_bs + 1):\n",
    "            start_i = i * validate_bs\n",
    "            end_i = start_i + validate_bs\n",
    "            #print(xb.shape)\n",
    "            xb = x_validate[start_i:end_i, :, :]\n",
    "            yb = y_validate[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss.append(loss_func(pred, yb))\n",
    "            acc.append(accuracy(pred, yb))\n",
    "\n",
    "        #print(loss)\n",
    "        valid_loss = sum(loss)\n",
    "    return valid_loss, sum(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #print('Training')\n",
    "        model.train()\n",
    "        for i in range((n_train - 1) // train_bs + 1):\n",
    "            \n",
    "            start_i = i * train_bs\n",
    "            end_i = start_i + train_bs\n",
    "            xb = x_train[start_i:end_i, :, :]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            #print('pred: ', pred, ' | yb: ', yb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "            #print('Acc: ', accuracy(pred, yb))\n",
    "        \n",
    "        ####validate at each epoch\n",
    "        #print('Validating')\n",
    "#         model.eval()\n",
    "#         loss = []\n",
    "#         acc = []\n",
    "#         with torch.no_grad():\n",
    "\n",
    "#             for i in range((n_validate - 1) // validate_bs + 1):\n",
    "#                 start_i = i * validate_bs\n",
    "#                 end_i = start_i + validate_bs\n",
    "#                 #print(xb.shape)\n",
    "#                 xb = x_validate[start_i:end_i, :, :]\n",
    "#                 yb = y_validate[start_i:end_i]\n",
    "#                 pred = model(xb)\n",
    "#                 loss.append(loss_func(pred, yb))\n",
    "#                 acc.append(accuracy(pred, yb))\n",
    "        \n",
    "#         #print(loss)\n",
    "#         valid_loss = sum(loss)/len(loss)\n",
    "        valid_loss, acc = validate(model)\n",
    "        \n",
    "        if (epoch + 1)% 10 == 0:\n",
    "            print('Epoch: ', epoch + 1, ' | Loss: ', valid_loss, ' | Accuracy: ', acc)\n",
    "    print('Train Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10  | Loss:  tensor(1.0906)  | Accuracy:  tensor(0.3440)\n",
      "Epoch:  20  | Loss:  tensor(1.0772)  | Accuracy:  tensor(0.6635)\n",
      "Epoch:  30  | Loss:  tensor(1.0638)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  40  | Loss:  tensor(1.0502)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  50  | Loss:  tensor(1.0361)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  60  | Loss:  tensor(1.0214)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  70  | Loss:  tensor(1.0060)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  80  | Loss:  tensor(0.9901)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  90  | Loss:  tensor(0.9736)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  100  | Loss:  tensor(0.9567)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  110  | Loss:  tensor(0.9398)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  120  | Loss:  tensor(0.9233)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  130  | Loss:  tensor(0.9076)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  140  | Loss:  tensor(0.8932)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  150  | Loss:  tensor(0.8805)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  160  | Loss:  tensor(0.8698)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  170  | Loss:  tensor(0.8611)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  180  | Loss:  tensor(0.8544)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  190  | Loss:  tensor(0.8493)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  200  | Loss:  tensor(0.8457)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  210  | Loss:  tensor(0.8432)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  220  | Loss:  tensor(0.8416)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  230  | Loss:  tensor(0.8405)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  240  | Loss:  tensor(0.8399)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  250  | Loss:  tensor(0.8395)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  260  | Loss:  tensor(0.8394)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  270  | Loss:  tensor(0.8393)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  280  | Loss:  tensor(0.8393)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  290  | Loss:  tensor(0.8394)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  300  | Loss:  tensor(0.8394)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  310  | Loss:  tensor(0.8395)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  320  | Loss:  tensor(0.8396)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  330  | Loss:  tensor(0.8396)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  340  | Loss:  tensor(0.8397)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  350  | Loss:  tensor(0.8397)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  360  | Loss:  tensor(0.8398)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  370  | Loss:  tensor(0.8398)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  380  | Loss:  tensor(0.8399)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  390  | Loss:  tensor(0.8399)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  400  | Loss:  tensor(0.8399)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  410  | Loss:  tensor(0.8399)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  420  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  430  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  440  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  450  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  460  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  470  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  480  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  490  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  500  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  510  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  520  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  530  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  540  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  550  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  560  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  570  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  580  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  590  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  600  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  610  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  620  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  630  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  640  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  650  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  660  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  670  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  680  | Loss:  tensor(0.8401)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  690  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  700  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  710  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  720  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  730  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  740  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  750  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  760  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  770  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  780  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  790  | Loss:  tensor(0.8400)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  800  | Loss:  tensor(0.8399)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  810  | Loss:  tensor(0.8399)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  820  | Loss:  tensor(0.8399)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  830  | Loss:  tensor(0.8399)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  840  | Loss:  tensor(0.8399)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  850  | Loss:  tensor(0.8399)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  860  | Loss:  tensor(0.8399)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  870  | Loss:  tensor(0.8399)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  880  | Loss:  tensor(0.8398)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  890  | Loss:  tensor(0.8398)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  900  | Loss:  tensor(0.8398)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  910  | Loss:  tensor(0.8398)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  920  | Loss:  tensor(0.8398)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  930  | Loss:  tensor(0.8398)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  940  | Loss:  tensor(0.8398)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  950  | Loss:  tensor(0.8398)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  960  | Loss:  tensor(0.8397)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  970  | Loss:  tensor(0.8397)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  980  | Loss:  tensor(0.8397)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  990  | Loss:  tensor(0.8397)  | Accuracy:  tensor(0.6857)\n",
      "Epoch:  1000  | Loss:  tensor(0.8397)  | Accuracy:  tensor(0.6857)\n",
      "Train Finished\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_save(save_path, epochs = epochs, lr = lr):\n",
    "    \n",
    "#     print('Training with epochs (', epochs, ') and lr (', lr, ')')\n",
    "#     model, opt = get_model(epochs = epochs, lr = lr)\n",
    "    \n",
    "#     fit()\n",
    "#     torch.save(model.state_dict(), save_path)\n",
    "\n",
    "\n",
    "##load trained model for evaluation\n",
    "save_path = 'trained_model.pt'\n",
    "torch.save(model, save_path)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8403), tensor(0.6857))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model class must be defined somewhere\n",
    "x = torch.load(save_path)\n",
    "validate(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
