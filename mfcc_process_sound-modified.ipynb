{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracts features from audio files and converts into numpy\n",
    "import librosa, pickle, random\n",
    "import numpy as np\n",
    "import os, re, csv, sys\n",
    "from datetime import datetime\n",
    "from random import shuffle\n",
    "import torch\n",
    "\n",
    "global hop_length, mfcc_len\n",
    "# global att, tar\n",
    "# Set the hop length; at 22050 Hz, 512 samples ~= 23ms\n",
    "hop_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## from Kagle tutorial. See https://www.kaggle.com/CVxTz/audio-data-augmentation\n",
    "def addWhiteNoise(audio):\n",
    "    noise = np.random.randn(len(audio))\n",
    "    return audio + 0.05*noise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## return a (flatten) one-D array of mfcc of an audio file\n",
    "# def getFlattenMFCC(audio_file):\n",
    "#     try:\n",
    "#         y, sr = librosa.load(audio_file)\n",
    "#     except FileNotFoundError:\n",
    "#         print('No such file or directory')\n",
    "\n",
    "#     # Compute MFCC features from the raw signal\n",
    "#     print(audio_file)\n",
    "#     return librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13).flatten()\n",
    "\n",
    "def getMFCC(audio_file):\n",
    "    #print ('audio_file in getFlattenChroma: ', audio_file)\n",
    "    \n",
    "    try: \n",
    "        y, sr = librosa.load(audio_file)\n",
    "    except FileNotFoundError:\n",
    "        print('No such file or directory')\n",
    "    return librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
    "\n",
    "\n",
    "def getChroma(audio_file):\n",
    "    #print ('audio_file in getFlattenChroma: ', audio_file)\n",
    "    \n",
    "    try: \n",
    "        y, sr = librosa.load(audio_file)\n",
    "    except FileNotFoundError:\n",
    "        print('No such file or directory')\n",
    "    #print('print audio_file inside getFlattenChroma: ', audio_file)\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "    return librosa.feature.chroma_cqt(y=y_harmonic,sr=sr)\n",
    "\n",
    "\n",
    "## return a list of 1-d array of chromagram padded with 0's of ALL audio files\n",
    "def getPaddedChroma(chroma):\n",
    "\n",
    "    ##pad arrays with 0's. Get arrays of size Max\n",
    "    max_col = max([x.shape[1] for x in chroma])\n",
    "    padded = [np.pad(x, [(0,0), (0, max_col - x.shape[1])], mode = 'constant') for x in chroma]\n",
    "\n",
    "    ##sanity check\n",
    "    is_shorter = sum([x.shape[1] - max_col for x in padded])\n",
    "    if is_shorter < 0:\n",
    "        print('not padded well')\n",
    "        return -1\n",
    "    else:\n",
    "        return padded\n",
    "    \n",
    "## return a list of 1-d array of MFCC padded with 0's of ALL audio files\n",
    "def getPaddedMFCC(mfcc):\n",
    "\n",
    "    print(mfcc)\n",
    "    ##pad arrays with 0's. Get arrays of size Max\n",
    "    max_col = max([x.shape[1] for x in mfcc])\n",
    "    padded = [np.pad(x, [(0,0), (0, max_col - x.shape[1])], mode = 'constant') for x in mfcc]\n",
    "\n",
    "    ##sanity check\n",
    "    is_shorter = sum([x.shape[1] - max_col for x in padded])\n",
    "    if is_shorter < 0:\n",
    "        print('not padded well')\n",
    "        return -1\n",
    "    else:\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_audio(raw_file_list):\n",
    "    \n",
    "    file_list = [x for x in raw_file_list if '.mp3' in x]\n",
    "    print('Processing ', len(file_list), ' files')\n",
    "\n",
    "    ##sample data to add whitenoise\n",
    "    ##########################\n",
    "    wnratio = 0.7\n",
    "    r = range(int(wnratio*len(file_list)))\n",
    "\n",
    "    sample_file_list = [random.choice(file_list) for i in r]\n",
    "\n",
    "    padded_mfcc_result = []\n",
    "    for s in sample_file_list:\n",
    "        y, sr = librosa.load(s)\n",
    "        wn_y = addWhiteNoise(y)\n",
    "        z = librosa.feature.mfcc(y=wn_y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
    "        padded_mfcc_result.append(z)\n",
    "    #getPaddedMFCC(padded_mfcc_result)\n",
    "    \n",
    "    ####MFCC\n",
    "#     mfcc = [getFlattenMFCC(f) for f in file_list]\n",
    "#     padded_mdcc, mfcc_len = getPaddedMFCC(mfcc)\n",
    "    \n",
    "    mfcc = [getMFCC(f) for f in file_list]\n",
    "    padded_mfcc = getPaddedMFCC(mfcc + padded_mfcc_result)\n",
    "    #print (\"padded_chroma: \", padded_chroma[0])\n",
    "    \n",
    "    \n",
    "    ### user cannot fix this\n",
    "#     assert (mfcc != -1 and chrom != -1), \"Audio process does not produce uniform format.\"\n",
    "\n",
    "    \n",
    "    \n",
    "    ## concatenate mfcc and chrom features\n",
    "#     attr_input = [np.hstack([m, c]) for m, c in zip(padded_mdcc, padded_chrom)]\n",
    "    attr_input = padded_mfcc\n",
    "    ## checking the final length\n",
    "    #print(len(mfcc[5]), len(chrom[5]), len(x[5]))\n",
    "    \n",
    "    \n",
    "    ##detect targets from sound names\n",
    "    p = re.compile('^[aeou]|[bcdfghjklmnpqrstwxyz]+(?=[aeiou])|nv|lv')\n",
    "    splt_file = [f.split('/')[-1] for f in file_list+sample_file_list]\n",
    "    \n",
    "    target_input = [p.match(f).group() for f in splt_file]\n",
    "    \n",
    "    print('Processing finished')\n",
    "    return attr_input, target_input\n",
    "\n",
    "##to process audio file for both validation and testing\n",
    "def process_validate_audio(file_list):\n",
    "    \n",
    "    print('Processing ', len(file_list), ' files')\n",
    "    \n",
    "    ####MFCC\n",
    "    mfcc = [getFlattenMFCC(f) for f in file_list]\n",
    "    padded_mdcc = getPaddedMFCC(mfcc)\n",
    "    chroma = [getFlattenChroma(f) for f in file_list]\n",
    "    padded_chroma = getPaddedChroma(chroma)\n",
    "    \n",
    "    ### user cannot fix this\n",
    "    assert (mfcc != -1 and chroma != -1), \"Audio process does not produce uniform format.\"\n",
    "\n",
    "    \n",
    "    \n",
    "    ## concatenate mfcc and chrom features\n",
    "    attr_input = [np.hstack([m, c]) for m, c in zip(padded_mdcc, padded_chroma)]\n",
    "    ## checking the final length\n",
    "    #print(len(mfcc[5]), len(chrom[5]), len(x[5]))\n",
    "\n",
    "\n",
    "    ##detect targets from sound names\n",
    "    p = re.compile('^[aeou]|[bcdfghjklmnpqrstwxyz]+(?=[aeiou])|nv|lv')\n",
    "    target_input = [p.match(f).group() for f in audio_files]\n",
    "    \n",
    "    print('Processing finished')\n",
    "    return attr_input, target_input     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_dir = '/Users/panchanok/Desktop/PyHack2019/PyHack2019/sound_samples/mix_samples/'\n",
    "# all_files = [audio_dir + d for d in os.listdir(audio_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# att, tar = process_train_audio(all_files)\n",
    "# att_file = open(r'TEST_S_X_ATT.pkl', 'wb')\n",
    "# pickle.dump(att, att_file)\n",
    "# att_file.close()\n",
    "# tar_file = open(r'TEST_S_X_TAR.pkl', 'wb')\n",
    "# pickle.dump(tar, tar_file)\n",
    "# tar_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #raw_file_list\n",
    "\n",
    "# file_list = [x for x in raw_file_list if '.mp3' in x]\n",
    "# print('Processing ', len(file_list), ' files')\n",
    "\n",
    "# ##sample data to add whitenoise\n",
    "# wnratio = 0.7\n",
    "# r = range(int(wnratio*len(file_list)))\n",
    "\n",
    "# sample_file_list = [random.choice(file_list) for i in r]\n",
    "\n",
    "# result = []\n",
    "# for s in sample_file_list:\n",
    "#     y, sr = librosa.load(s)\n",
    "#     wn_y = addWhiteNoise(y)\n",
    "#     z = librosa.feature.mfcc(y=wn_y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
    "#     result.append(z)\n",
    "# getPaddedMFCC(result)\n",
    "# #result.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######################################\n",
    "##Compare before and after adding WN #\n",
    "######################################\n",
    "######################################\n",
    "######################################\n",
    "import IPython.display as ipd\n",
    "\n",
    "audio_dir = '/Users/panchanok/Desktop/PyHack2019/PyHack2019/sound_samples/xs/'\n",
    "all_files = [audio_dir + d for d in os.listdir(audio_dir)][1:10]\n",
    "shuffle(all_files)\n",
    "\n",
    "print(all_files)\n",
    "file_list = [x for x in all_files if '.mp3' in x]\n",
    "print('Processing ', len(file_list), ' files')\n",
    "\n",
    "##sample data to add whitenoise\n",
    "wnratio = 0.7\n",
    "r = range(int(wnratio*len(file_list)))\n",
    "print(r)\n",
    "sample_file_list = [random.choice(file_list) for i in r]\n",
    "sample_audio = [librosa.load(f) for f in sample_file_list]\n",
    "\n",
    "y, sr = librosa.load(sample_file_list[0])\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveplot(y, sr=sr)\n",
    "ipd.Audio(y, rate = 22050)\n",
    "\n",
    "\n",
    "\n",
    "# wn_audio = [addWhiteNoise(a) for a in sample_audio]\n",
    "# mfcc_wn_audio = [librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13) for y in wn_audio]\n",
    "\n",
    "# p_mfcc_wn_audio = [getPaddedMFCC(f) for f in mfcc_wn_audio]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = addWhiteNoise(y)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveplot(new, sr=sr)\n",
    "ipd.Audio(new, rate = 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  9840  files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing finished\n"
     ]
    }
   ],
   "source": [
    "#audio_dir = '/Users/athicha/Desktop/PyHack2019/sound_samples/train/'\n",
    "audio_dir = '/Users/panchanok/Desktop/PyHack2019/sound/tone_perfect/'\n",
    "all_files = [audio_dir + d for d in os.listdir(audio_dir)]\n",
    "#print(len(all_files))\n",
    "shuffle(all_files)\n",
    "\n",
    "att, tar = process_train_audio(all_files)\n",
    "att_file = open(r'mfcc_noise_attr.pkl', 'wb')\n",
    "pickle.dump(att, att_file)\n",
    "att_file.close()\n",
    "\n",
    "\n",
    "tar_file = open(r'mfcc_noise_tar.pkl', 'wb')\n",
    "pickle.dump(tar, tar_file)\n",
    "tar_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model class must be defined somewhere\n",
    "# model = torch.load('mfcc_m_1000_dr0.5_wc0.001_lr1e-05.pt')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16728"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16728"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
