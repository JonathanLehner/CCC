{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model using built-in library from Pytorch.\n",
    "This code closelt follows nn_tutorial notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, torch, math\n",
    "\n",
    "### read in target and attr input files\n",
    "target_file = 'target_in.csv'\n",
    "attr_file = 'attr_in.csv'\n",
    "\n",
    "with open(target_file, 'r') as f: \n",
    "    reader = csv.reader(f)\n",
    "    target_input_list = list(reader)\n",
    "    \n",
    "with open(attr_file, 'r') as f: \n",
    "    reader = csv.reader(f)\n",
    "    attr_input_list = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target_input_list is in list of lists. We want it to be a list of characters.\n",
    "attr_input is also a list of string lists. We want it to be a list of float lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_input = [''.join(sublist) for sublist in target_input_list]\n",
    "attr_input = [[float(x) for x in t] for t in attr_input_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current target is a string of the actual sound. We want it to be number of that it is computable when we train the model. We convert as the following:\n",
    "'s': 0\n",
    "'x': 1\n",
    "'neither': 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_y = []\n",
    "for i in target_input:\n",
    "    if i == 's': \n",
    "        coded_y.append(0)\n",
    "    elif i == 'x':\n",
    "        coded_y.append(1)\n",
    "    else:\n",
    "        coded_y.append(2)\n",
    "\n",
    "\n",
    "## convert arrays and lists into tensors for training\n",
    "x_train, y_train = map(torch.tensor, (attr_input, coded_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train and y_train should be in the right shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([101, 3545]) torch.Size([101])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are three classes, we set D_out to 3. n is total number of instances and c is the number of attributes in each instance. We use a loss function from torch.nn.functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,c = x_train.shape\n",
    "D_out = 3\n",
    "\n",
    "##Xavier initialisation (x 1/sqrt(n))\n",
    "weights = torch.randn(c, D_out) / math.sqrt(c)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(D_out, requires_grad = True)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a simple linear combination from model() and check if the shapes are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([101, 3545]) torch.Size([101, 3]) torch.Size([101])\n"
     ]
    }
   ],
   "source": [
    "preds = model(x_train) \n",
    "print(x_train.shape, preds.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running loss function on the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(26.5361, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(preds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(c, D_out) / math.sqrt(c))\n",
    "        self.bias = nn.Parameter(torch.zeros(D_out))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias\n",
    "    \n",
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "tensor(119048.2969, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "                \n",
    "    print('Done')\n",
    "\n",
    "fit()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "++++ torch.FloatTensor <class 'torch.Size'>\n",
      "---- tensor(525371.2500, grad_fn=<NllLossBackward>)\n",
      "weights: <class 'torch.Tensor'>\n",
      "grad:  <class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'zero'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-aa3a1d5b431e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mbias\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'zero'"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "bs = 11\n",
    "epochs = 2\n",
    "lr = 0.5 ## learning rate\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        print(i)\n",
    "        #set_trace()\n",
    "        start_i = i*bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "#         print(pred.shape)\n",
    "#         print(xb.shape)\n",
    "#         print(pred)\n",
    "#         print(xb)\n",
    "        print('++++', pred.type(), type(yb.shape))\n",
    "        loss = loss_func(pred, yb)\n",
    "        print('----',loss)\n",
    "        \n",
    "        #print('loss', loss)\n",
    "        #loss.backward()\n",
    "        \n",
    "        ## Get one output from multiple outputs by\n",
    "        ## average all losses across a batch.\n",
    "        loss.backward()\n",
    "        print('weights:', type(weights.grad))\n",
    "        with torch.no_grad():\n",
    "            print('grad: ', type(weights.grad))\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero()\n",
    "            bias.grad.zero()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
